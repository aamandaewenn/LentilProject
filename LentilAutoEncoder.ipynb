{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path, path2=None):\n",
    "    metadata = pd.read_csv(path)\n",
    "    image_paths = metadata.Image.values\n",
    "     \n",
    "    if path2 is not None:\n",
    "        lbp = pd.read_csv(path2)\n",
    "    \n",
    "        features = np.concatenate([\n",
    "            column_extractor(lbp.Lbp.tolist()),\n",
    "            column_extractor(metadata.Color.tolist()),\n",
    "            column_extractor(metadata.Glcm.tolist()),\n",
    "            column_extractor(metadata.Gccrop.tolist()),\n",
    "            column_extractor(metadata.Lbp.tolist()),\n",
    "            ], axis=1)\n",
    "    else:\n",
    "        features = np.concatenate([\n",
    "        column_extractor(metadata.Color.tolist()),\n",
    "        column_extractor(metadata.Glcm.tolist()),\n",
    "        column_extractor(metadata.Gccrop.tolist()),\n",
    "        column_extractor(metadata.Lbp.tolist()),\n",
    "        ], axis=1)\n",
    "        \n",
    "    columns = [f'F_{i:0>3}' for i in range(len(features))]\n",
    "    features = pd.DataFrame(features, columns)\n",
    "    feat2drop = features.loc[:, features.var(axis=0) == 0].columns\n",
    "    features.drop(feat2drop, axis=1, inplace=True)\n",
    "    return (image_paths, features)\n",
    "\n",
    "def column_extractor(feature_list):\n",
    "    feature_list = [\n",
    "        list(map(float, item.strip('[]').split(',')))\n",
    "        for item in feature_list\n",
    "    ]\n",
    "    return np.array(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LentilDataset(Dataset): \n",
    "    def __init__(self, features): \n",
    "        super(LentilDataset, self).__init__()\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, item): \n",
    "        feature = self.features[item]\n",
    "        \n",
    "        feature = (feature - feature.mean()) / (feature.max() - feature.min())\n",
    "        feature = torch.tensor(feature).float()\n",
    "            \n",
    "        return item, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, df = data_loader('ImageProcessingPipeline/data/processed/real_features.csv')\n",
    "features = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, valid_features= np.split(features, [int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1272, 1835)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = LentilDataset(train_features)\n",
    "valid_dataset = LentilDataset(valid_features)\n",
    "train_dataset.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_loader = DataLoader(\\n    test_dataset,\\n    batch_size=BATCH_SIZE, \\n    shuffle=False,\\n    num_workers=0\\n)\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataloaders. \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([32, 1835])\n",
      "torch.Size([24, 1835])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    items, features = data\n",
    "    print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set device to  cpu\n"
     ]
    }
   ],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, feature_size):\n",
    "        super().__init__()\n",
    "        self.feature_size = feature_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(feature_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "        \n",
    "        self.decoder =nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, feature_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Set device to ', device)\n",
    "model = MLPModel(feature_size=1835)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=1835, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ReLU()\n",
      "    (1): Linear(in_features=32, out_features=64, bias=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=128, out_features=1024, bias=True)\n",
      "    (8): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=1024, out_features=1835, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Criterion Definition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train --> Epoch: 0, Loss: 0.09217337006703019.\n",
      "Valid --> Epoch: 0, Loss: 0.01986293252557516.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 1, Loss: 0.02797339386306703.\n",
      "Valid --> Epoch: 1, Loss: 0.013309492263942957.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 2, Loss: 0.012136029719840735.\n",
      "Valid --> Epoch: 2, Loss: 0.010616773553192616.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 3, Loss: 0.00792184517486021.\n",
      "Valid --> Epoch: 3, Loss: 0.0076739702373743056.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 4, Loss: 0.005250345729291439.\n",
      "Valid --> Epoch: 4, Loss: 0.0048788778716698285.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 5, Loss: 0.004063761315774172.\n",
      "Valid --> Epoch: 5, Loss: 0.0034601550316438077.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 6, Loss: 0.0031480810430366546.\n",
      "Valid --> Epoch: 6, Loss: 0.0027961299289017915.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 7, Loss: 0.0028385468205669893.\n",
      "Valid --> Epoch: 7, Loss: 0.0024804100510664286.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 8, Loss: 0.002304408940835856.\n",
      "Valid --> Epoch: 8, Loss: 0.002055810659658164.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 9, Loss: 0.001892778041656129.\n",
      "Valid --> Epoch: 9, Loss: 0.0016157727222889662.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 10, Loss: 0.001672823866829276.\n",
      "Valid --> Epoch: 10, Loss: 0.0013873265706934035.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 11, Loss: 0.0015566677087917925.\n",
      "Valid --> Epoch: 11, Loss: 0.0012004994961898774.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 12, Loss: 0.0013462462244206107.\n",
      "Valid --> Epoch: 12, Loss: 0.0011347638792358339.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 13, Loss: 0.0012802684897906147.\n",
      "Valid --> Epoch: 13, Loss: 0.0010133733740076423.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 14, Loss: 0.001165042528009508.\n",
      "Valid --> Epoch: 14, Loss: 0.0008232237305492163.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 15, Loss: 0.0009906431441777386.\n",
      "Valid --> Epoch: 15, Loss: 0.0008516993199009449.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 16, Loss: 0.000972395516873803.\n",
      "Valid --> Epoch: 16, Loss: 0.0006999672739766538.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 17, Loss: 0.001000470131111797.\n",
      "Valid --> Epoch: 17, Loss: 0.0007556507305707782.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 18, Loss: 0.0007851666581700556.\n",
      "Valid --> Epoch: 18, Loss: 0.0005897071008803323.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 19, Loss: 0.00065673322242219.\n",
      "Valid --> Epoch: 19, Loss: 0.0005445987510029227.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 20, Loss: 0.000788355485565262.\n",
      "Valid --> Epoch: 20, Loss: 0.0005392220831708982.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 21, Loss: 0.0006092342489864677.\n",
      "Valid --> Epoch: 21, Loss: 0.0004918565857224167.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 22, Loss: 0.000610843639151426.\n",
      "Valid --> Epoch: 22, Loss: 0.0004598217608872801.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 23, Loss: 0.0005194084136746824.\n",
      "Valid --> Epoch: 23, Loss: 0.00042514576634857806.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 24, Loss: 0.0005220460683631245.\n",
      "Valid --> Epoch: 24, Loss: 0.00040144914528355.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 25, Loss: 0.0005529340072826016.\n",
      "Valid --> Epoch: 25, Loss: 0.0003893359680660069.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 26, Loss: 0.0004646371518902015.\n",
      "Valid --> Epoch: 26, Loss: 0.0003786742308875546.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 27, Loss: 0.0005595281472778879.\n",
      "Valid --> Epoch: 27, Loss: 0.00038152170018292966.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 28, Loss: 0.0005008703403291293.\n",
      "Valid --> Epoch: 28, Loss: 0.0003514440351864323.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 29, Loss: 0.0004167147330008447.\n",
      "Valid --> Epoch: 29, Loss: 0.00031635326886316764.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 30, Loss: 0.00038543005903193264.\n",
      "Valid --> Epoch: 30, Loss: 0.00030076886905590073.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 31, Loss: 0.0004892621935141505.\n",
      "Valid --> Epoch: 31, Loss: 0.00040397133270744236.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 32, Loss: 0.00038727434257452843.\n",
      "Valid --> Epoch: 32, Loss: 0.0003376747452421114.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 33, Loss: 0.00040407730957667807.\n",
      "Valid --> Epoch: 33, Loss: 0.00031307133031077683.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 34, Loss: 0.0003303313180367695.\n",
      "Valid --> Epoch: 34, Loss: 0.0002747199047007598.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 35, Loss: 0.00034602555679157375.\n",
      "Valid --> Epoch: 35, Loss: 0.00028422350878827275.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 36, Loss: 0.00033481915488664526.\n",
      "Valid --> Epoch: 36, Loss: 0.00025764038000488656.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 37, Loss: 0.000321602039548452.\n",
      "Valid --> Epoch: 37, Loss: 0.00026543003477854656.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 38, Loss: 0.00027478670635900927.\n",
      "Valid --> Epoch: 38, Loss: 0.0002321522290003486.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 39, Loss: 0.00029702977735723834.\n",
      "Valid --> Epoch: 39, Loss: 0.00021553118276642635.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 40, Loss: 0.0002752822798356647.\n",
      "Valid --> Epoch: 40, Loss: 0.0002260212626424618.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 41, Loss: 0.0002746028396359179.\n",
      "Valid --> Epoch: 41, Loss: 0.0002340853010537103.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 42, Loss: 0.00023875307742855513.\n",
      "Valid --> Epoch: 42, Loss: 0.00019621638057287782.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 43, Loss: 0.0002889110593969235.\n",
      "Valid --> Epoch: 43, Loss: 0.00019457589005469344.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 44, Loss: 0.00022551146794285158.\n",
      "Valid --> Epoch: 44, Loss: 0.00020583810983225702.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 45, Loss: 0.0002508168381609721.\n",
      "Valid --> Epoch: 45, Loss: 0.00019853224221151323.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 46, Loss: 0.0002452544904372189.\n",
      "Valid --> Epoch: 46, Loss: 0.00020664968105847946.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 47, Loss: 0.0002171673873817781.\n",
      "Valid --> Epoch: 47, Loss: 0.00017530491677462124.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 48, Loss: 0.0001987207964702975.\n",
      "Valid --> Epoch: 48, Loss: 0.00016645068244542928.\n",
      "--------------------------------------------------\n",
      "Train --> Epoch: 49, Loss: 0.00020580230120685882.\n",
      "Valid --> Epoch: 49, Loss: 0.0001655012463743333.\n",
      "--------------------------------------------------\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "MAXEPOCH = 50\n",
    "\n",
    "train_loss_tracking = []\n",
    "vlaid_loss_tracking = []\n",
    "\n",
    "for epoch in range(MAXEPOCH):\n",
    "    # Change the development mode to the training mode. \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    running_steps = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        _, features = data\n",
    "        features=features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(features)\n",
    "        \n",
    "\n",
    "        loss = criterion(outputs, features.clone())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        running_steps += 1\n",
    "    train_loss = running_loss / running_steps\n",
    "    train_loss_tracking.append(train_loss)\n",
    "\n",
    "    print(f'Train --> Epoch: {epoch}, Loss: {train_loss}.')\n",
    "\n",
    "    # Evaluate the model for this epoch. \n",
    "    # Change the development mode to the evaluating mode. \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "\n",
    "        running_steps = 0\n",
    "        for i, data in enumerate(valid_loader, 0):\n",
    "            _, features = data\n",
    "            features=features.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, features)\n",
    "\n",
    "           \n",
    "\n",
    "            running_loss += loss.cpu().item()\n",
    "  \n",
    "            running_steps += 1\n",
    "        valid_loss = running_loss / running_steps\n",
    "        vlaid_loss_tracking.append(valid_loss)\n",
    "\n",
    "        print(f'Valid --> Epoch: {epoch}, Loss: {valid_loss}.')\n",
    "        print('-' * 50)\n",
    "        \n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the Loss Values along different epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_loss_tracking)), train_loss_tracking, 'r-', label='Train Loss')\n",
    "plt.plot(range(len(vlaid_loss_tracking)), vlaid_loss_tracking, 'g-', label='Valid Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, load, and test the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to where you want to save model to\n",
    "PATH = 'Experiments/MLP_Model1/MLPModel1_1024_32.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model. \n",
    "checkpoint = torch.load('Experiments/MLP_Model1/MLPModel1_1024_32.pth')\n",
    "# Define the base model.\n",
    "model = MLPModel(feature_size=1835)\n",
    "# Load the weights. \n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/processed/real_regions/0000_16401081_1.png</td>\n",
       "      <td>-0.239764</td>\n",
       "      <td>-1.019379</td>\n",
       "      <td>-0.346687</td>\n",
       "      <td>-1.134223</td>\n",
       "      <td>0.953623</td>\n",
       "      <td>-0.489345</td>\n",
       "      <td>-0.500263</td>\n",
       "      <td>-0.908257</td>\n",
       "      <td>0.135639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.518105</td>\n",
       "      <td>-0.686634</td>\n",
       "      <td>-0.482850</td>\n",
       "      <td>-0.560879</td>\n",
       "      <td>-0.277670</td>\n",
       "      <td>-0.278536</td>\n",
       "      <td>-0.320225</td>\n",
       "      <td>0.746123</td>\n",
       "      <td>-0.379762</td>\n",
       "      <td>-0.373733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/processed/real_regions/0001_16401081_10.png</td>\n",
       "      <td>-0.465878</td>\n",
       "      <td>-0.973409</td>\n",
       "      <td>-0.228608</td>\n",
       "      <td>-1.277515</td>\n",
       "      <td>0.958979</td>\n",
       "      <td>-0.448639</td>\n",
       "      <td>-0.619784</td>\n",
       "      <td>-1.081189</td>\n",
       "      <td>0.261915</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413013</td>\n",
       "      <td>-0.635301</td>\n",
       "      <td>-0.547232</td>\n",
       "      <td>-0.643058</td>\n",
       "      <td>-0.317868</td>\n",
       "      <td>-0.257788</td>\n",
       "      <td>-0.287477</td>\n",
       "      <td>0.691416</td>\n",
       "      <td>-0.361773</td>\n",
       "      <td>-0.383534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/processed/real_regions/0002_16401081_11.png</td>\n",
       "      <td>-0.452348</td>\n",
       "      <td>-0.635310</td>\n",
       "      <td>-0.420549</td>\n",
       "      <td>-0.897135</td>\n",
       "      <td>1.046878</td>\n",
       "      <td>-0.420234</td>\n",
       "      <td>-0.497287</td>\n",
       "      <td>-0.635895</td>\n",
       "      <td>0.231592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317345</td>\n",
       "      <td>-0.202304</td>\n",
       "      <td>-0.195658</td>\n",
       "      <td>-1.002852</td>\n",
       "      <td>-0.383824</td>\n",
       "      <td>-0.279611</td>\n",
       "      <td>-0.427565</td>\n",
       "      <td>0.749669</td>\n",
       "      <td>-0.290503</td>\n",
       "      <td>-0.356184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/processed/real_regions/0003_16401081_12.png</td>\n",
       "      <td>-0.437938</td>\n",
       "      <td>-1.007361</td>\n",
       "      <td>-0.303397</td>\n",
       "      <td>-1.376005</td>\n",
       "      <td>1.003717</td>\n",
       "      <td>-0.523574</td>\n",
       "      <td>-0.653041</td>\n",
       "      <td>-1.114258</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500840</td>\n",
       "      <td>-0.653560</td>\n",
       "      <td>-0.514833</td>\n",
       "      <td>-0.583210</td>\n",
       "      <td>-0.198052</td>\n",
       "      <td>-0.295081</td>\n",
       "      <td>-0.228383</td>\n",
       "      <td>0.726096</td>\n",
       "      <td>-0.345267</td>\n",
       "      <td>-0.428249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/processed/real_regions/0004_16401081_14.png</td>\n",
       "      <td>-0.369338</td>\n",
       "      <td>-0.802248</td>\n",
       "      <td>-0.212950</td>\n",
       "      <td>-1.032188</td>\n",
       "      <td>0.803689</td>\n",
       "      <td>-0.318665</td>\n",
       "      <td>-0.328542</td>\n",
       "      <td>-1.230685</td>\n",
       "      <td>0.144174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.590844</td>\n",
       "      <td>-0.262846</td>\n",
       "      <td>-0.531957</td>\n",
       "      <td>-0.523357</td>\n",
       "      <td>-0.299751</td>\n",
       "      <td>-0.522190</td>\n",
       "      <td>-0.378554</td>\n",
       "      <td>0.634659</td>\n",
       "      <td>-0.087520</td>\n",
       "      <td>-0.117878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Image         0         1  \\\n",
       "0   data/processed/real_regions/0000_16401081_1.png -0.239764 -1.019379   \n",
       "1  data/processed/real_regions/0001_16401081_10.png -0.465878 -0.973409   \n",
       "2  data/processed/real_regions/0002_16401081_11.png -0.452348 -0.635310   \n",
       "3  data/processed/real_regions/0003_16401081_12.png -0.437938 -1.007361   \n",
       "4  data/processed/real_regions/0004_16401081_14.png -0.369338 -0.802248   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -0.346687 -1.134223  0.953623 -0.489345 -0.500263 -0.908257  0.135639  ...   \n",
       "1 -0.228608 -1.277515  0.958979 -0.448639 -0.619784 -1.081189  0.261915  ...   \n",
       "2 -0.420549 -0.897135  1.046878 -0.420234 -0.497287 -0.635895  0.231592  ...   \n",
       "3 -0.303397 -1.376005  1.003717 -0.523574 -0.653041 -1.114258  0.238400  ...   \n",
       "4 -0.212950 -1.032188  0.803689 -0.318665 -0.328542 -1.230685  0.144174  ...   \n",
       "\n",
       "         10        11        12        13        14        15        16  \\\n",
       "0 -0.518105 -0.686634 -0.482850 -0.560879 -0.277670 -0.278536 -0.320225   \n",
       "1 -0.413013 -0.635301 -0.547232 -0.643058 -0.317868 -0.257788 -0.287477   \n",
       "2 -0.317345 -0.202304 -0.195658 -1.002852 -0.383824 -0.279611 -0.427565   \n",
       "3 -0.500840 -0.653560 -0.514833 -0.583210 -0.198052 -0.295081 -0.228383   \n",
       "4 -0.590844 -0.262846 -0.531957 -0.523357 -0.299751 -0.522190 -0.378554   \n",
       "\n",
       "         17        18        19  \n",
       "0  0.746123 -0.379762 -0.373733  \n",
       "1  0.691416 -0.361773 -0.383534  \n",
       "2  0.749669 -0.290503 -0.356184  \n",
       "3  0.726096 -0.345267 -0.428249  \n",
       "4  0.634659 -0.087520 -0.117878  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the features\n",
    "features = df.to_numpy()\n",
    "\n",
    "feature_dataset = LentilDataset(features)\n",
    "feature_loader = DataLoader(\n",
    "    feature_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "output_features = []\n",
    "output_items = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(feature_loader):\n",
    "        items, features = data\n",
    "        features = features.to(device)\n",
    "        output = model.encoder(features)\n",
    "        output_features.extend(output.cpu().numpy().tolist())\n",
    "        output_items.extend(items.numpy().tolist())\n",
    "\n",
    "paths = np.array(paths)\n",
    "encoded_features = pd.DataFrame(output_features)\n",
    "encoded_features.head()\n",
    "\n",
    "path_df = pd.DataFrame(paths[output_items], columns=['Image'])\n",
    "\n",
    "encoded_features = pd.concat([path_df, encoded_features], axis=1)\n",
    "encoded_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path to where you want encoded features saved to\n",
    "encoded_features.to_csv('Experiments/MLP_Model1//encoded_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode New Lentil Features (a new dataset not used above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model. \n",
    "checkpoint = torch.load('Experiments/MLP_Model1//DeepModel1_1024_32.pth')\n",
    "# Define the base model.\n",
    "model = MLPModel(feature_size=1835)\n",
    "# Load the weights. \n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c8/9q_gdr094zx7cdf4lp5gc5z000glxp/T/ipykernel_31881/2007393249.py:9: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Features0010_x', 'Features0018_x', 'Features0009_x', 'Features0016_x', 'Features0001_x', 'Features0007_x', 'Features0008_x', 'Features0005_x', 'Features0015_x', 'Features0002_x', 'Features0000_x', 'Features0017_x', 'Features0004_x', 'Features0006_x', 'Features0013_x', 'Features0019_x', 'Features0014_x', 'Features0003_x', 'Features0012_x', 'Features0011_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  features_df = features_df.merge(colours_df, on=\"Path\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features0000_x</th>\n",
       "      <th>Features0001_x</th>\n",
       "      <th>Features0002_x</th>\n",
       "      <th>Features0003_x</th>\n",
       "      <th>Features0004_x</th>\n",
       "      <th>Features0005_x</th>\n",
       "      <th>Features0006_x</th>\n",
       "      <th>Features0007_x</th>\n",
       "      <th>Features0008_x</th>\n",
       "      <th>Features0009_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Features0245_y</th>\n",
       "      <th>Features0246_y</th>\n",
       "      <th>Features0247_y</th>\n",
       "      <th>Features0248_y</th>\n",
       "      <th>Features0249_y</th>\n",
       "      <th>Features0250_y</th>\n",
       "      <th>Features0251_y</th>\n",
       "      <th>Features0252_y</th>\n",
       "      <th>Features0253_y</th>\n",
       "      <th>Features0254_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.519531</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1835 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Features0000_x  Features0001_x  Features0002_x  Features0003_x  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.519531        0.519531        0.519531        0.519531   \n",
       "2        0.000000        0.000000        0.000000        0.000000   \n",
       "3        0.023438        0.023438        0.023438        0.023438   \n",
       "4        0.000000        0.000000        0.000000        0.000000   \n",
       "\n",
       "   Features0004_x  Features0005_x  Features0006_x  Features0007_x  \\\n",
       "0        0.000000        0.000000        0.000000        0.000000   \n",
       "1        0.519531        0.519531        0.640625        0.640625   \n",
       "2        0.000000        0.000000        0.000000        0.000000   \n",
       "3        0.023438        0.023438        0.042969        0.042969   \n",
       "4        0.000000        0.000000        0.000000        0.000000   \n",
       "\n",
       "   Features0008_x  Features0009_x  ...  Features0245_y  Features0246_y  \\\n",
       "0        0.000000        0.000000  ...               0               0   \n",
       "1        0.640625        0.640625  ...               0               0   \n",
       "2        0.000000        0.000000  ...               0               0   \n",
       "3        0.042969        0.042969  ...               0               0   \n",
       "4        0.000000        0.000000  ...               0               0   \n",
       "\n",
       "   Features0247_y  Features0248_y  Features0249_y  Features0250_y  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   Features0251_y  Features0252_y  Features0253_y  Features0254_y  \n",
       "0               0               0               0               0  \n",
       "1               0               0               0               0  \n",
       "2               0               0               0               0  \n",
       "3               0               0               0               0  \n",
       "4               0               0               0               0  \n",
       "\n",
       "[5 rows x 1835 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "glcm_df = pd.read_csv('metadata/new/glcm.csv')\n",
    "gcc_df = pd.read_csv('metadata/new/gccrop.csv')\n",
    "lbp_df= pd.read_csv('metadata/new/lbp.csv')\n",
    "colours_df = pd.read_csv('metadata/new/colours.csv')\n",
    "\n",
    "features_df = glcm_df.merge(gcc_df, on='Path')\n",
    "features_df = features_df.merge(lbp_df, on=\"Path\")\n",
    "features_df = features_df.merge(colours_df, on=\"Path\")\n",
    "\n",
    "paths = features_df[\"Path\"]\n",
    "features_df = features_df.iloc[:, 1:]\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../Desktop/New_Lentils/brown-marbled-705...</td>\n",
       "      <td>-0.515018</td>\n",
       "      <td>-1.109583</td>\n",
       "      <td>-1.503038</td>\n",
       "      <td>2.639349</td>\n",
       "      <td>0.557113</td>\n",
       "      <td>2.706753</td>\n",
       "      <td>1.673643</td>\n",
       "      <td>0.612868</td>\n",
       "      <td>1.201330</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.692108</td>\n",
       "      <td>-2.533953</td>\n",
       "      <td>2.315301</td>\n",
       "      <td>-0.396628</td>\n",
       "      <td>-1.022466</td>\n",
       "      <td>-1.316653</td>\n",
       "      <td>0.726105</td>\n",
       "      <td>4.032653</td>\n",
       "      <td>-1.241618</td>\n",
       "      <td>-0.554506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../Desktop/New_Lentils/brown-marbled-705...</td>\n",
       "      <td>-0.519995</td>\n",
       "      <td>-1.118364</td>\n",
       "      <td>-1.432239</td>\n",
       "      <td>2.636273</td>\n",
       "      <td>0.586249</td>\n",
       "      <td>2.748581</td>\n",
       "      <td>1.675357</td>\n",
       "      <td>0.627036</td>\n",
       "      <td>1.225140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.677867</td>\n",
       "      <td>-2.492073</td>\n",
       "      <td>2.360404</td>\n",
       "      <td>-0.360366</td>\n",
       "      <td>-0.971100</td>\n",
       "      <td>-1.279968</td>\n",
       "      <td>0.697061</td>\n",
       "      <td>4.044016</td>\n",
       "      <td>-1.182493</td>\n",
       "      <td>-0.541687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../Desktop/New_Lentils/brown-marbled-705...</td>\n",
       "      <td>-0.516553</td>\n",
       "      <td>-1.107391</td>\n",
       "      <td>-1.481799</td>\n",
       "      <td>2.649528</td>\n",
       "      <td>0.585725</td>\n",
       "      <td>2.735550</td>\n",
       "      <td>1.663095</td>\n",
       "      <td>0.633206</td>\n",
       "      <td>1.225655</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.676208</td>\n",
       "      <td>-2.519499</td>\n",
       "      <td>2.338145</td>\n",
       "      <td>-0.379308</td>\n",
       "      <td>-0.995481</td>\n",
       "      <td>-1.305787</td>\n",
       "      <td>0.708677</td>\n",
       "      <td>4.048633</td>\n",
       "      <td>-1.210562</td>\n",
       "      <td>-0.543196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../Desktop/New_Lentils/brown-marbled-705...</td>\n",
       "      <td>-0.533245</td>\n",
       "      <td>-1.112543</td>\n",
       "      <td>-1.465940</td>\n",
       "      <td>2.637363</td>\n",
       "      <td>0.565678</td>\n",
       "      <td>2.722894</td>\n",
       "      <td>1.675655</td>\n",
       "      <td>0.621697</td>\n",
       "      <td>1.207232</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.690161</td>\n",
       "      <td>-2.507153</td>\n",
       "      <td>2.339423</td>\n",
       "      <td>-0.376351</td>\n",
       "      <td>-0.997499</td>\n",
       "      <td>-1.299004</td>\n",
       "      <td>0.728099</td>\n",
       "      <td>4.029946</td>\n",
       "      <td>-1.210553</td>\n",
       "      <td>-0.539367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../../Desktop/New_Lentils/brown-marbled-705...</td>\n",
       "      <td>-0.495666</td>\n",
       "      <td>-1.102903</td>\n",
       "      <td>-1.572198</td>\n",
       "      <td>2.640399</td>\n",
       "      <td>0.514628</td>\n",
       "      <td>2.711427</td>\n",
       "      <td>1.703423</td>\n",
       "      <td>0.597742</td>\n",
       "      <td>1.203958</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.712073</td>\n",
       "      <td>-2.588059</td>\n",
       "      <td>2.300238</td>\n",
       "      <td>-0.431820</td>\n",
       "      <td>-1.057143</td>\n",
       "      <td>-1.365184</td>\n",
       "      <td>0.739574</td>\n",
       "      <td>4.029470</td>\n",
       "      <td>-1.315311</td>\n",
       "      <td>-0.565174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image         0         1  \\\n",
       "0  ../../../Desktop/New_Lentils/brown-marbled-705... -0.515018 -1.109583   \n",
       "1  ../../../Desktop/New_Lentils/brown-marbled-705... -0.519995 -1.118364   \n",
       "2  ../../../Desktop/New_Lentils/brown-marbled-705... -0.516553 -1.107391   \n",
       "3  ../../../Desktop/New_Lentils/brown-marbled-705... -0.533245 -1.112543   \n",
       "4  ../../../Desktop/New_Lentils/brown-marbled-705... -0.495666 -1.102903   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -1.503038  2.639349  0.557113  2.706753  1.673643  0.612868  1.201330  ...   \n",
       "1 -1.432239  2.636273  0.586249  2.748581  1.675357  0.627036  1.225140  ...   \n",
       "2 -1.481799  2.649528  0.585725  2.735550  1.663095  0.633206  1.225655  ...   \n",
       "3 -1.465940  2.637363  0.565678  2.722894  1.675655  0.621697  1.207232  ...   \n",
       "4 -1.572198  2.640399  0.514628  2.711427  1.703423  0.597742  1.203958  ...   \n",
       "\n",
       "         22        23        24        25        26        27        28  \\\n",
       "0 -1.692108 -2.533953  2.315301 -0.396628 -1.022466 -1.316653  0.726105   \n",
       "1 -1.677867 -2.492073  2.360404 -0.360366 -0.971100 -1.279968  0.697061   \n",
       "2 -1.676208 -2.519499  2.338145 -0.379308 -0.995481 -1.305787  0.708677   \n",
       "3 -1.690161 -2.507153  2.339423 -0.376351 -0.997499 -1.299004  0.728099   \n",
       "4 -1.712073 -2.588059  2.300238 -0.431820 -1.057143 -1.365184  0.739574   \n",
       "\n",
       "         29        30        31  \n",
       "0  4.032653 -1.241618 -0.554506  \n",
       "1  4.044016 -1.182493 -0.541687  \n",
       "2  4.048633 -1.210562 -0.543196  \n",
       "3  4.029946 -1.210553 -0.539367  \n",
       "4  4.029470 -1.315311 -0.565174  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the features\n",
    "features = features_df.to_numpy()\n",
    "\n",
    "feature_dataset = LentilDataset(features)\n",
    "feature_loader = DataLoader(\n",
    "    feature_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "output_features = []\n",
    "output_items = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(feature_loader):\n",
    "        items, features = data\n",
    "        features = features.to(device)\n",
    "        output = model.encoder(features)\n",
    "        output_features.extend(output.cpu().numpy().tolist())\n",
    "        output_items.extend(items.numpy().tolist())\n",
    "\n",
    "paths = np.array(paths)\n",
    "encoded_features = pd.DataFrame(output_features)\n",
    "encoded_features.head()\n",
    "\n",
    "path_df = pd.DataFrame(paths[output_items], columns=['Image'])\n",
    "\n",
    "encoded_features = pd.concat([path_df, encoded_features], axis=1)\n",
    "encoded_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoded features\n",
    "encoded_features.to_csv('Experiments/MLP_Model1//new_encoded_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
